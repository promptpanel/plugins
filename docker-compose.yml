version: "3.9"
services:
  promptpanel:
    image: promptpanel/promptpanel:latest
    container_name: promptpanel_development
    restart: always
    ports:
      - 4000:4000
    volumes:
      ## Community plugins
      - ./completion_chat:/app/plugins/completion_chat
      - ./completion_document:/app/plugins/completion_document
      - ./ollama_chat:/app/plugins/ollama_chat
      - ./ollama_document:/app/plugins/ollama_document
      - ./llm_chat:/app/plugins/llm_chat
      - ./llm_document:/app/plugins/llm_document
      ## Your new plugins
      - ./my-first-plugin:/app/plugins/my-first-plugin
    environment:
      PROMPT_MODE: DEVELOPMENT ## Applies changes server-side as you work.
      PROMPT_DEV_INSTALL_REQS: ENABLED ## Installs requirements on container startup.
      PROMPT_OLLAMA_HOST: http://ollama:11434
  ## Ollama is optional for local inference.
  ## You can remove this service & the `PROMPT_OLLAMA_HOST` environment variable in order to disable local inference.
  ollama:
    image: ollama/ollama:latest
    container_name: ollama
    restart: always
